{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rVKkIWIsRUb",
        "outputId": "3618a67b-d9d1-4821-850f-6a5fac0b13c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from opendatasets) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from opendatasets) (8.1.3)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (from opendatasets) (1.5.13)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2.27.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle->opendatasets) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: aaryan30\n",
            "Your Kaggle Key: ··········\n",
            "Downloading liveliness.zip to ./liveliness\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 141M/141M [00:02<00:00, 69.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/pjsingh/liveliness\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------- profile_detection ---------------------------------------\n",
        "detect_frontal_face = 'profile_detection/haarcascades/haarcascade_frontalface_alt.xml'\n",
        "detect_perfil_face = 'profile_detection/haarcascades/haarcascade_profileface.xml'\n",
        "\n",
        "# Parametros del modelo, la imagen se debe convertir a una de tamaño 48x48 en escala de grises\n",
        "w,h = 48,48\n",
        "rgb = False\n",
        "# definir la relacion de aspecto del ojo EAT\n",
        "# definir el numero de frames consecutivos que debe estar por debajo del umbral\n",
        "EYE_AR_THRESH = 0.23 #baseline\n",
        "EYE_AR_CONSEC_FRAMES = 1\n",
        "\n",
        "# eye landmarks\n",
        "eye_landmarks = \"blink_detection/model_landmarks/shape_predictor_68_face_landmarks.dat\""
      ],
      "metadata": {
        "id": "zj-_j_B-EB47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "import ops\n",
        "\n",
        "\n",
        "def get_vgg16_conv5(input, params):\n",
        "    layers = edict()\n",
        "\n",
        "    layers.conv1_1 = ops.conv2D(input=input, shape=(3, 3, 64), name='conv1_1', params=params)\n",
        "    layers.conv1_1_relu = ops.activate(input=layers.conv1_1, name='conv1_1_relu', act_type='relu')\n",
        "    layers.conv1_2 = ops.conv2D(input=layers.conv1_1_relu, shape=(3, 3, 64), name='conv1_2', params=params)\n",
        "    layers.conv1_2_relu = ops.activate(input=layers.conv1_2, name='conv1_2_relu', act_type='relu')\n",
        "    layers.pool1 = ops.max_pool(input=layers.conv1_2_relu, name='pool1')\n",
        "\n",
        "    layers.conv2_1 = ops.conv2D(input=layers.pool1, shape=(3, 3, 128), name='conv2_1', params=params)\n",
        "    layers.conv2_1_relu = ops.activate(input=layers.conv2_1, name='conv2_1_relu', act_type='relu')\n",
        "    layers.conv2_2 = ops.conv2D(input=layers.conv2_1_relu, shape=(3, 3, 128), name='conv2_2', params=params)\n",
        "    layers.conv2_2_relu = ops.activate(input=layers.conv2_2, name='conv2_2_relu', act_type='relu')\n",
        "    layers.pool2 = ops.max_pool(input=layers.conv2_2_relu, name='pool2')\n",
        "\n",
        "    layers.conv3_1 = ops.conv2D(input=layers.pool2, shape=(3, 3, 256), name='conv3_1', params=params)\n",
        "    layers.conv3_1_relu = ops.activate(input=layers.conv3_1, name='conv3_1_relu', act_type='relu')\n",
        "    layers.conv3_2 = ops.conv2D(input=layers.conv3_1_relu, shape=(3, 3, 256), name='conv3_2', params=params)\n",
        "    layers.conv3_2_relu = ops.activate(input=layers.conv3_2, name='conv3_2_relu', act_type='relu')\n",
        "    layers.conv3_3 = ops.conv2D(input=layers.conv3_2_relu, shape=(3, 3, 256), name='conv3_3', params=params)\n",
        "    layers.conv3_3_relu = ops.activate(input=layers.conv3_3, name='conv3_3_relu', act_type='relu')\n",
        "    layers.pool3 = ops.max_pool(input=layers.conv3_3_relu, name='pool3')\n",
        "\n",
        "    layers.conv4_1 = ops.conv2D(input=layers.pool3, shape=(3, 3, 512), name='conv4_1', params=params)\n",
        "    layers.conv4_1_relu = ops.activate(input=layers.conv4_1, name='conv4_1_relu', act_type='relu')\n",
        "    layers.conv4_2 = ops.conv2D(input=layers.conv4_1_relu, shape=(3, 3, 512), name='conv4_2', params=params)\n",
        "    layers.conv4_2_relu = ops.activate(input=layers.conv4_2, name='conv4_2_relu', act_type='relu')\n",
        "    layers.conv4_3 = ops.conv2D(input=layers.conv4_2_relu, shape=(3, 3, 512), name='conv4_3', params=params)\n",
        "    layers.conv4_3_relu = ops.activate(input=layers.conv4_3, name='conv4_3_relu', act_type='relu')\n",
        "    layers.pool4 = ops.max_pool(input=layers.conv4_3_relu, name='pool4')\n",
        "\n",
        "    layers.conv5_1 = ops.conv2D(input=layers.pool4, shape=(3, 3, 512), name='conv5_1', params=params)\n",
        "    layers.conv5_1_relu = ops.activate(input=layers.conv5_1, name='conv5_1_relu', act_type='relu')\n",
        "    layers.conv5_2 = ops.conv2D(input=layers.conv5_1_relu, shape=(3, 3, 512), name='conv5_2', params=params)\n",
        "    layers.conv5_2_relu = ops.activate(input=layers.conv5_2, name='conv5_2_relu', act_type='relu')\n",
        "    layers.conv5_3 = ops.conv2D(input=layers.conv5_2_relu, shape=(3, 3, 512), name='conv5_3', params=params)\n",
        "    layers.conv5_3_relu = ops.activate(input=layers.conv5_3, name='conv5_3_relu', act_type='relu')\n",
        "\n",
        "    return layers\n",
        "\n",
        "def get_vgg16_pool5(input, params):\n",
        "    layers = get_vgg16_conv5(input, params)\n",
        "    layers.pool5 = ops.max_pool(input=layers.conv5_3_relu, name='pool5')\n",
        "\n",
        "    return layers\n",
        "\n",
        "def get_prob(input, params, num_class=1000, is_train=True):\n",
        "    # Get pool5\n",
        "    layers = get_vgg16_pool5(input, params)\n",
        "    layers.fc6 = ops.fully_connected(input=layers.pool5, num_neuron=4096, name='fc6', params=params)\n",
        "    if is_train:\n",
        "        layers.fc6 = tf.nn.dropout(layers.fc6, keep_prob=0.5)\n",
        "    layers.fc6_relu = ops.activate(input=layers.fc6, act_type='relu', name='fc6_relu')\n",
        "    layers.fc7 = ops.fully_connected(input=layers.fc6_relu, num_neuron=4096, name='fc7', params=params)\n",
        "    if is_train:\n",
        "        layers.fc7 = tf.nn.dropout(layers.fc7, keep_prob=0.5)\n",
        "    layers.fc7_relu = ops.activate(input=layers.fc7, act_type='relu', name='fc7_relu')\n",
        "    layers.fc8 = ops.fully_connected(input=layers.fc7_relu, num_neuron=num_class, name='fc8', params=params)\n",
        "    layers.prob = tf.nn.softmax(layers.fc8)\n",
        "    return layers"
      ],
      "metadata": {
        "id": "2SXxGl3JFd4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2, os\n",
        "import matplotlib\n",
        "import sys\n",
        "import warnings\n",
        "import dlib\n",
        "matplotlib.use('Agg')\n",
        "from tqdm import tqdm\n",
        "sys.path.append(os.path.dirname(os.path.dirname(__file__)))\n",
        "import pickle\n",
        "from py_utils.face_utils import lib\n",
        "from py_utils.vid_utils import proc_vid as pv\n",
        "from py_utils.plot_utils import plot\n",
        "\n",
        "\n",
        "class Solu(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_vid_path,\n",
        "                 output_height=300,\n",
        "                 ):\n",
        "        # Input video\n",
        "        self.input_vid_path = input_vid_path\n",
        "        # parse video\n",
        "        print('Parsing video {}'.format(str(self.input_vid_path)))\n",
        "        self.imgs, self.frame_num, self.fps, self.img_w, self.img_h = pv.parse_vid(str(self.input_vid_path))\n",
        "        if len(self.imgs) != self.frame_num:\n",
        "            warnings.warn('Frame number is not consistent with the number of images in video...')\n",
        "            self.frame_num = len(self.imgs)\n",
        "        print('Eye blinking solution is building...')\n",
        "\n",
        "        self._set_up_dlib()\n",
        "\n",
        "        self.output_height = output_height\n",
        "        factor = float(self.output_height) / self.img_h\n",
        "\n",
        "        # Resize imgs for final video generation\n",
        "        # Resize self.imgs according to self.output_height\n",
        "        self.aligned_imgs = []\n",
        "        self.left_eyes = []\n",
        "        self.right_eyes = []\n",
        "\n",
        "        self.resized_imgs = []\n",
        "        print('face aligning...')\n",
        "        for i, im in enumerate(tqdm(self.imgs)):\n",
        "            face_cache = lib.align(im[:, :, (2,1,0)], self.front_face_detector, self.lmark_predictor)\n",
        "            if len(face_cache) == 0:\n",
        "                self.left_eyes.append(None)\n",
        "                self.right_eyes.append(None)\n",
        "                continue\n",
        "\n",
        "            if len(face_cache) > 1:\n",
        "                raise ValueError('{} faces are in image, we only support one face in image.')\n",
        "\n",
        "            aligned_img, aligned_shapes_cur = lib.get_aligned_face_and_landmarks(im, face_cache)\n",
        "            # crop eyes\n",
        "            leye, reye = lib.crop_eye(aligned_img[0], aligned_shapes_cur[0])\n",
        "            self.left_eyes.append(leye)\n",
        "            self.right_eyes.append(reye)\n",
        "            im_resized = cv2.resize(im, None, None, fx=factor, fy=factor)\n",
        "            self.resized_imgs.append(im_resized)\n",
        "\n",
        "        # For visualize\n",
        "        self.plot_vis_list = []\n",
        "        self.total_eye1_prob = []\n",
        "        self.total_eye2_prob = []\n",
        "\n",
        "    def _set_up_dlib(self):\n",
        "        # Note that CUDA dir should be /usr/local/cuda and without AVX intructions\n",
        "        pwd = os.path.dirname(os.path.abspath(__file__))\n",
        "        # self.cnn_face_detector = dlib.cnn_face_detection_model_v1(pwd + '/mmod_human_face_detector.dat')\n",
        "        self.front_face_detector = dlib.get_frontal_face_detector()\n",
        "        self.lmark_predictor = dlib.shape_predictor(pwd + '/dlib_model/shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "    def gen_videos(self, out_dir, tag=''):\n",
        "        vid_name = os.path.basename(self.input_vid_path)\n",
        "        out_path = os.path.join(out_dir, tag + '_' + vid_name)\n",
        "        print('Generating video: {}'.format(out_path))\n",
        "        # Output folder\n",
        "        if not os.path.exists(os.path.dirname(out_dir)):\n",
        "            os.makedirs(os.path.dirname(out_dir))\n",
        "\n",
        "        final_list = []\n",
        "        for i in tqdm(range(self.frame_num)):\n",
        "            final_vis = np.concatenate([self.resized_imgs[i], self.plot_vis_list[i]], axis=1)\n",
        "            final_list.append(final_vis)\n",
        "        pv.gen_vid(out_path, final_list, self.fps)\n",
        "\n",
        "    def get_eye_by_fid(self, i):\n",
        "        eye1, eye2 = self.left_eyes[i], self.right_eyes[i]\n",
        "        return eye1, eye2\n",
        "\n",
        "    def push_eye_prob(self, eye1_prob, eye2_prob):\n",
        "        self.total_eye1_prob.append(eye1_prob)\n",
        "        self.total_eye2_prob.append(eye2_prob)\n",
        "\n",
        "    def plot_by_fid(self, i):\n",
        "        # Vis plots\n",
        "        max_X = self.frame_num / self.fps\n",
        "        params = {}\n",
        "        params['title'] = 'Eye-state-probability'\n",
        "        params['colors'] = ['b-']\n",
        "        params['markers'] = [None]\n",
        "        params['linewidth'] = 3\n",
        "        params['markersize'] = None\n",
        "        params['figsize'] = None\n",
        "\n",
        "        x_axis = np.arange(self.frame_num) / self.fps\n",
        "        # Vis plots\n",
        "        prob_plot_1 = plot.draw2D([x_axis[:i + 1]],\n",
        "                                [self.total_eye1_prob],\n",
        "                                order=[''],\n",
        "                                xname='time',\n",
        "                                yname='eye state',\n",
        "                                params=params,\n",
        "                                xlim=[0, max_X],\n",
        "                                ylim=[-1, 2])\n",
        "\n",
        "        prob_plot_2 = plot.draw2D([x_axis[:i + 1]],\n",
        "                                [self.total_eye2_prob],\n",
        "                                order=[''],\n",
        "                                xname='time',\n",
        "                                yname='eye state',\n",
        "                                params=params,\n",
        "                                xlim=[0, max_X],\n",
        "                                ylim=[-1, 2])\n",
        "\n",
        "        vis = np.concatenate([prob_plot_1, prob_plot_2], axis=1)\n",
        "        scale = float(self.output_height) / vis.shape[0]\n",
        "        # Resize plot size to same size with video\n",
        "        vis = cv2.resize(vis, None, None, fx=scale, fy=scale)\n",
        "        self.plot_vis_list.append(vis)\n",
        "        return self.plot_vis_list"
      ],
      "metadata": {
        "id": "NPYvCXKXFv6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os, cv2\n",
        "from deep_base.ops import get_restore_var_list\n",
        "import yaml, os\n",
        "from easydict import EasyDict as edict\n",
        "pwd = os.path.dirname(__file__)\n",
        "\n",
        "\n",
        "class Solver(object):\n",
        "    \"\"\"\n",
        "    Solver for training and testing\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 sess,\n",
        "                 net,\n",
        "                 mode='cnn'):\n",
        "        cfg_file = os.path.join(pwd, 'blink_{}.yml'.format(mode))\n",
        "        with open(cfg_file, 'r') as f:\n",
        "            cfg = edict(yaml.load(f))\n",
        "\n",
        "        self.sess = sess\n",
        "        self.net = net\n",
        "        self.cfg = cfg\n",
        "        self.mode = mode\n",
        "\n",
        "    def init(self):\n",
        "        cfg = self.cfg\n",
        "        self.img_size = cfg.IMG_SIZE\n",
        "        pwd = os.path.dirname(os.path.abspath(__file__))\n",
        "        self.summary_dir = os.path.join(pwd, cfg.SUMMARY_DIR)\n",
        "        if not os.path.exists(self.summary_dir):\n",
        "            os.makedirs(self.summary_dir)\n",
        "\n",
        "        self.model_dir = os.path.join(pwd, cfg.MODEL_DIR)\n",
        "        if not os.path.exists(self.model_dir):\n",
        "            os.makedirs(self.model_dir)\n",
        "        self.model_path = os.path.join(self.model_dir, 'model.ckpt')\n",
        "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "        self.saver = tf.train.Saver(max_to_keep=5)\n",
        "        # initialize the graph\n",
        "        if self.net.is_train:\n",
        "            self.num_epoch = cfg.TRAIN.NUM_EPOCH\n",
        "            self.learning_rate = cfg.TRAIN.LEARNING_RATE\n",
        "            self.decay_rate = cfg.TRAIN.DECAY_RATE\n",
        "            self.decay_step = cfg.TRAIN.DECAY_STEP\n",
        "            self.net.loss()\n",
        "            self.set_optimizer()\n",
        "            # Add summary\n",
        "            self.loss_summary = tf.summary.scalar('loss_summary', self.net.total_loss)\n",
        "            self.lr_summary = tf.summary.scalar('learning_rate_summary', self.LR)\n",
        "            self.summary = tf.summary.merge([self.loss_summary, self.lr_summary])\n",
        "            self.writer = tf.summary.FileWriter(self.summary_dir, self.sess.graph)\n",
        "\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.load()\n",
        "\n",
        "    def train(self, *args):\n",
        "        if self.mode == 'cnn':\n",
        "            return self.train_cnn(images=args[0], labels=args[1])\n",
        "        elif self.mode == 'lrcn':\n",
        "            return self.train_lrcn(seq_tensor=args[0],\n",
        "                                   len_list=args[1],\n",
        "                                   state_list=args[2])\n",
        "        else:\n",
        "            raise ValueError('We only support mode = [cnn, lrcn]...')\n",
        "\n",
        "    def test(self, *args):\n",
        "        if self.mode == 'cnn':\n",
        "            return self.test_cnn(images=args[0])\n",
        "        elif self.mode == 'lrcn':\n",
        "            return self.test_lrcn(seq_tensor=args[0],\n",
        "                                   len_list=args[1])\n",
        "        else:\n",
        "            raise ValueError('We only support mode = [cnn, lrcn]...')\n",
        "\n",
        "    def test_cnn(self, images):\n",
        "        # Check input size\n",
        "        for i, im in enumerate(images):\n",
        "            images[i] = cv2.resize(im, (self.img_size[0], self.img_size[1]))\n",
        "\n",
        "        feed_dict = {\n",
        "            self.net.input: images,\n",
        "        }\n",
        "        fetch_list = [\n",
        "            self.net.prob,\n",
        "        ]\n",
        "        return self.sess.run(fetch_list, feed_dict=feed_dict)\n",
        "\n",
        "    def train_cnn(self, images, labels):\n",
        "        feed_dict = {\n",
        "            self.net.input: images,\n",
        "            self.net.gt: labels\n",
        "        }\n",
        "        fetch_list = [\n",
        "            self.train_op,\n",
        "            self.summary,\n",
        "            self.net.prob,\n",
        "            self.net.net_loss,\n",
        "        ]\n",
        "        return self.sess.run(fetch_list, feed_dict=feed_dict)\n",
        "\n",
        "    def test_lrcn(self, seq_tensor, len_list):\n",
        "        feed_dict = {\n",
        "            self.net.input: seq_tensor,\n",
        "            self.net.seq_len: len_list,\n",
        "        }\n",
        "        fetch_list = [\n",
        "            self.net.prob,\n",
        "        ]\n",
        "        return self.sess.run(fetch_list, feed_dict=feed_dict)\n",
        "\n",
        "    def train_lrcn(self, seq_tensor, len_list, state_list):\n",
        "        feed_dict = {\n",
        "            self.net.input: seq_tensor,\n",
        "            self.net.seq_len: len_list,\n",
        "            self.net.eye_state_gt: state_list\n",
        "        }\n",
        "        fetch_list = [\n",
        "            self.train_op,\n",
        "            self.summary,\n",
        "            self.net.prob,\n",
        "            self.net.net_loss,\n",
        "\n",
        "        ]\n",
        "        return self.sess.run(fetch_list, feed_dict=feed_dict)\n",
        "\n",
        "    def save(self, step):\n",
        "        \"\"\" Save checkpoints \"\"\"\n",
        "        save_path = self.saver.save(self.sess, self.model_path, global_step=step)\n",
        "        print('Model {} saved in file.'.format(save_path))\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"Load weights from checkpoint\"\"\"\n",
        "        if os.path.isfile(self.model_path + '.meta'):\n",
        "            variables_to_restore = get_restore_var_list(self.model_path)\n",
        "            restorer = tf.train.Saver(variables_to_restore)\n",
        "            restorer.restore(self.sess, self.model_path)\n",
        "            print('Loading checkpoint {}'.format(self.model_path))\n",
        "        else:\n",
        "            print('Loading failed.')\n",
        "\n",
        "    def set_optimizer(self):\n",
        "        # Set learning rate decay\n",
        "        self.LR = tf.train.exponential_decay(\n",
        "            learning_rate=self.learning_rate,\n",
        "            global_step=self.global_step,\n",
        "            decay_steps=self.decay_step,\n",
        "            decay_rate=self.decay_rate,\n",
        "            staircase=True\n",
        "        )\n",
        "        if self.cfg.TRAIN.METHOD == 'SGD':\n",
        "            optimizer = tf.train.GradientDescentOptimizer(\n",
        "                learning_rate=self.LR,\n",
        "            )\n",
        "        elif self.cfg.TRAIN.METHOD == 'Adam':\n",
        "            optimizer = tf.train.AdamOptimizer(\n",
        "                learning_rate=self.LR,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError('We only support [SGD, Adam] right now...')\n",
        "\n",
        "        self.train_op = optimizer.minimize(\n",
        "            loss=self.net.total_loss,\n",
        "            global_step=self.global_step,\n",
        "            var_list=None)"
      ],
      "metadata": {
        "id": "PLF4ci3fF8yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ykaY7nvF85t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import config as cfg\n",
        "import dlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "from imutils import face_utils\n",
        "from scipy.spatial import distance as dist\n",
        "\n",
        "\n",
        "\n",
        "class eye_blink_detector():\n",
        "    def __init__(self):\n",
        "        # cargar modelo para deteccion de puntos de ojos\n",
        "        self.predictor_eyes = dlib.shape_predictor(cfg.eye_landmarks)\n",
        "\n",
        "    def eye_blink(self,gray,rect,COUNTER,TOTAL):\n",
        "        (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
        "        (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
        "        # determine the facial landmarks for the face region, then\n",
        "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
        "        # array\n",
        "        shape = self.predictor_eyes(gray, rect)\n",
        "        shape = face_utils.shape_to_np(shape)\n",
        "        # extract the left and right eye coordinates, then use the\n",
        "        # coordinates to compute the eye aspect ratio for both eyes\n",
        "        leftEye = shape[lStart:lEnd]\n",
        "        rightEye = shape[rStart:rEnd]\n",
        "        leftEAR = self.eye_aspect_ratio(leftEye)\n",
        "        rightEAR = self.eye_aspect_ratio(rightEye)\n",
        "        # average the eye aspect ratio together for both eyes\n",
        "        ear = (leftEAR + rightEAR) / 2.0\n",
        "        # check to see if the eye aspect ratio is below the blink\n",
        "        # threshold, and if so, increment the blink frame counter\n",
        "        if ear < cfg.EYE_AR_THRESH:\n",
        "            COUNTER += 1\n",
        "        # otherwise, the eye aspect ratio is not below the blink\n",
        "        # threshold\n",
        "        else:\n",
        "            # if the eyes were closed for a sufficient number of\n",
        "            # then increment the total number of blinks\n",
        "            if COUNTER >= cfg.EYE_AR_CONSEC_FRAMES:\n",
        "                TOTAL += 1\n",
        "            # reset the eye frame counter\n",
        "            COUNTER = 0\n",
        "        return COUNTER,TOTAL\n",
        "\n",
        "    def eye_aspect_ratio(self,eye):\n",
        "        # compute the euclidean distances between the two sets of\n",
        "        # vertical eye landmarks (x, y)-coordinates\n",
        "        A = dist.euclidean(eye[1], eye[5])\n",
        "        B = dist.euclidean(eye[2], eye[4])\n",
        "        # compute the euclidean distance between the horizontal\n",
        "        # eye landmark (x, y)-coordinates\n",
        "        C = dist.euclidean(eye[0], eye[3])\n",
        "        # compute the eye aspect ratio\n",
        "        ear = (A + B) / (2.0 * C)\n",
        "        # return the eye aspect ratio\n",
        "        return ear\n"
      ],
      "metadata": {
        "id": "z8wVA6cu3Xbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os, cv2\n",
        "from deep_base.ops import get_restore_var_list\n",
        "import yaml, os\n",
        "from easydict import EasyDict as edict\n",
        "pwd = os.path.dirname(__file__)\n",
        "\n",
        "\n",
        "class Solver(object):\n",
        "    \"\"\"\n",
        "    Solver for training and testing\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 sess,\n",
        "                 net,\n",
        "                 mode='cnn'):\n",
        "        cfg_file = os.path.join(pwd, 'blink_{}.yml'.format(mode))\n",
        "        with open(cfg_file, 'r') as f:\n",
        "            cfg = edict(yaml.load(f))\n",
        "\n",
        "        self.sess = sess\n",
        "        self.net = net\n",
        "        self.cfg = cfg\n",
        "        self.mode = mode\n",
        "\n",
        "    def init(self):\n",
        "        cfg = self.cfg\n",
        "        self.img_size = cfg.IMG_SIZE\n",
        "        pwd = os.path.dirname(os.path.abspath(__file__))\n",
        "        self.summary_dir = os.path.join(pwd, cfg.SUMMARY_DIR)\n",
        "        if not os.path.exists(self.summary_dir):\n",
        "            os.makedirs(self.summary_dir)\n",
        "\n",
        "        self.model_dir = os.path.join(pwd, cfg.MODEL_DIR)\n",
        "        if not os.path.exists(self.model_dir):\n",
        "            os.makedirs(self.model_dir)\n",
        "        self.model_path = os.path.join(self.model_dir, 'model.ckpt')\n",
        "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "        self.saver = tf.train.Saver(max_to_keep=5)\n",
        "        # initialize the graph\n",
        "        if self.net.is_train:\n",
        "            self.num_epoch = cfg.TRAIN.NUM_EPOCH\n",
        "            self.learning_rate = cfg.TRAIN.LEARNING_RATE\n",
        "            self.decay_rate = cfg.TRAIN.DECAY_RATE\n",
        "            self.decay_step = cfg.TRAIN.DECAY_STEP\n",
        "            self.net.loss()\n",
        "            self.set_optimizer()\n",
        "            # Add summary\n",
        "            self.loss_summary = tf.summary.scalar('loss_summary', self.net.total_loss)\n",
        "            self.lr_summary = tf.summary.scalar('learning_rate_summary', self.LR)\n",
        "            self.summary = tf.summary.merge([self.loss_summary, self.lr_summary])\n",
        "            self.writer = tf.summary.FileWriter(self.summary_dir, self.sess.graph)\n",
        "\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.load()\n",
        "\n",
        "    def train(self, *args):\n",
        "        if self.mode == 'cnn':\n",
        "            return self.train_cnn(images=args[0], labels=args[1])\n",
        "        elif self.mode == 'lrcn':\n",
        "            return self.train_lrcn(seq_tensor=args[0],\n",
        "                                   len_list=args[1],\n",
        "                                   state_list=args[2])\n",
        "        else:\n",
        "            raise ValueError('We only support mode = [cnn, lrcn]...')\n",
        "\n",
        "    def test(self, *args):\n",
        "        if self.mode == 'cnn':\n",
        "            return self.test_cnn(images=args[0])\n",
        "        elif self.mode == 'lrcn':\n",
        "            return self.test_lrcn(seq_tensor=args[0],\n",
        "                                   len_list=args[1])\n",
        "        else:\n",
        "            raise ValueError('We only support mode = [cnn, lrcn]...')\n",
        "\n",
        "    def test_cnn(self, images):\n",
        "        # Check input size\n",
        "        for i, im in enumerate(images):\n",
        "            images[i] = cv2.resize(im, (self.img_size[0], self.img_size[1]))\n",
        "\n",
        "        feed_dict = {\n",
        "            self.net.input: images,\n",
        "        }\n",
        "        fetch_list = [\n",
        "            self.net.prob,\n",
        "        ]\n",
        "        return self.sess.run(fetch_list, feed_dict=feed_dict)\n",
        "\n",
        "    def train_cnn(self, images, labels):\n",
        "        feed_dict = {\n",
        "            self.net.input: images,\n",
        "            self.net.gt: labels\n",
        "        }\n",
        "        fetch_list = [\n",
        "            self.train_op,\n",
        "            self.summary,\n",
        "            self.net.prob,\n",
        "            self.net.net_loss,\n",
        "        ]\n",
        "        return self.sess.run(fetch_list, feed_dict=feed_dict)\n",
        "\n",
        "    def test_lrcn(self, seq_tensor, len_list):\n",
        "        feed_dict = {\n",
        "            self.net.input: seq_tensor,\n",
        "            self.net.seq_len: len_list,\n",
        "        }\n",
        "        fetch_list = [\n",
        "            self.net.prob,\n",
        "        ]\n",
        "        return self.sess.run(fetch_list, feed_dict=feed_dict)\n",
        "\n",
        "    def train_lrcn(self, seq_tensor, len_list, state_list):\n",
        "        feed_dict = {\n",
        "            self.net.input: seq_tensor,\n",
        "            self.net.seq_len: len_list,\n",
        "            self.net.eye_state_gt: state_list\n",
        "        }\n",
        "        fetch_list = [\n",
        "            self.train_op,\n",
        "            self.summary,\n",
        "            self.net.prob,\n",
        "            self.net.net_loss,\n",
        "\n",
        "        ]\n",
        "        return self.sess.run(fetch_list, feed_dict=feed_dict)\n",
        "\n",
        "    def save(self, step):\n",
        "        \"\"\" Save checkpoints \"\"\"\n",
        "        save_path = self.saver.save(self.sess, self.model_path, global_step=step)\n",
        "        print('Model {} saved in file.'.format(save_path))\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"Load weights from checkpoint\"\"\"\n",
        "        if os.path.isfile(self.model_path + '.meta'):\n",
        "            variables_to_restore = get_restore_var_list(self.model_path)\n",
        "            restorer = tf.train.Saver(variables_to_restore)\n",
        "            restorer.restore(self.sess, self.model_path)\n",
        "            print('Loading checkpoint {}'.format(self.model_path))\n",
        "        else:\n",
        "            print('Loading failed.')\n",
        "\n",
        "    def set_optimizer(self):\n",
        "        # Set learning rate decay\n",
        "        self.LR = tf.train.exponential_decay(\n",
        "            learning_rate=self.learning_rate,\n",
        "            global_step=self.global_step,\n",
        "            decay_steps=self.decay_step,\n",
        "            decay_rate=self.decay_rate,\n",
        "            staircase=True\n",
        "        )\n",
        "        if self.cfg.TRAIN.METHOD == 'SGD':\n",
        "            optimizer = tf.train.GradientDescentOptimizer(\n",
        "                learning_rate=self.LR,\n",
        "            )\n",
        "        elif self.cfg.TRAIN.METHOD == 'Adam':\n",
        "            optimizer = tf.train.AdamOptimizer(\n",
        "                learning_rate=self.LR,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError('We only support [SGD, Adam] right now...')\n",
        "\n",
        "        self.train_op = optimizer.minimize(\n",
        "            loss=self.net.total_loss,\n",
        "            global_step=self.global_step,\n",
        "            var_list=None)\n"
      ],
      "metadata": {
        "id": "wmnxisYtEFl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize Video result with Labels\n",
        "import imutils\n",
        "from imutils.video import FileVideoStream, VideoStream\n",
        "vs = VideoStream(src=0).start()\n",
        "\n",
        "frame = imutils.resize(frame, width=450)\n",
        "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "rects = eye_blink_detector(gray, 0) #Key Model\n",
        "for rect in rects:\n",
        "\t\tshape = Solver(gray, rect)\n",
        "\t\tshape = face_utils.shape_to_np(shape)\n",
        "\n",
        "\t\t# extract the left and right eye coordinates, then use the\n",
        "\t\tleftEye = shape[lStart:lEnd]\n",
        "\t\trightEye = shape[rStart:rEnd]\n",
        "\t\t# visualize each of the eyes\n",
        "\t\tleftEyeHull = cv2.convexHull(leftEye)\n",
        "\t\trightEyeHull = cv2.convexHull(rightEye)\n",
        "\t\tcv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
        "\t\tcv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
        "\t\t# check to see if the eye aspect ratio is below the blink\n",
        "\t\t# threshold, and if so, increment the blink frame counter\n",
        "\t\tif ear < EYE_AR_THRESH:\n",
        "\t\t\tCOUNTER += 1\n",
        "    else\n",
        "      COUNTER = 0\n",
        "\n",
        "cv2.putText(frame, \"Blinks: {}\".format(TOTAL), (10, 30),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)'\n",
        "\n",
        "cv2.imshow(\"Frame\", frame)\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "vs.stop()"
      ],
      "metadata": {
        "id": "3sAGLSWd6Ayk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Blink Frequency Graph\n",
        "slopes.append((eye_heights[i]-eye_heights[i+1]))\n",
        "avg_slope = np.sum(slopes)/len(slopes) #average slope\n",
        "min_slope = min(slopes) #minimum slope\n",
        "\n",
        "print(abs(avg_slope-min_slope))\n",
        "\n",
        "plt.axline(xy1=(0,avg_slope),slope=0)\n",
        "plt.axline(xy1=(0,min_slope),slope=0)\n",
        "plt.plot(slopes)\n",
        "plt.plot(eye_heights)\n",
        "plt.scatter(x_axis, eye_heights)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XsHZvtj853uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! jupyter nbconvert --to html /content/Inoculi_liveliness1_0ipynb.ipynb"
      ],
      "metadata": {
        "id": "Vs0hgAFusf4s",
        "outputId": "89d68d28-77e7-4b88-b200-9d1868fdd974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/Inoculi_liveliness1_0ipynb.ipynb to html\n",
            "[NbConvertApp] Writing 712165 bytes to /content/Inoculi_liveliness1_0ipynb.html\n"
          ]
        }
      ]
    }
  ]
}